Journal of Mathematical Sciences, Vol. 199, No. 2, May, 2014

RANDOM DETERMINANTS, MIXED VOLUMES OF ELLIPSOIDS, AND ZEROS OF
GAUSSIAN RANDOM FIELDS
D. Zaporozhetsâˆ— and Z. Kabluchkoâ€ 

UDC 519.2+514

Consider a d Ã— d matrix M whose rows are independent, centered, nondegenerate Gaussian vectors Î¾1 , . . . , Î¾d with
covariance matrices Î£1 , . . . , Î£d . Denote by Ei the dispersion ellipsoid of Î¾i : Ei = {x âˆˆ Rd : x Î£âˆ’1
i x â‰¤ 1}. We
show that
d!
Vd (E1 , . . . , Ed ),
E | det M | =
(2Ï€)d/2
where Vd (Â·, . . . , Â·) denotes the mixed volume. We also generalize this result to the case of rectangular matrices. As
a direct corollary, we get an analytic expression for the mixed volume of d arbitrary ellipsoids in Rd .
As another application, we consider a smooth, centered, nondegenerate Gaussian random field X = (X1 , . . . , Xk ) :
of the intensity of zeros of X in terms
Rd â†’ Rk . Using the Kacâ€“Rice formula, we obtain a geometric interpretation
âˆš
of the mixed volume of dispersion ellipsoids of the gradients of Xi / VarXi . This relates zero sets of equations to
mixed volumes in a way which is reminiscent of the well-known Bernstein theorem about the number of solutions of
a typical system of algebraic equations. Bibliography: 10 titles.

1. Main results
1.1. Random determinant and mixed volume of ellipsoids. Consider independent, centered, nondegenerate Gaussian random vectors Î¾1 , . . . , Î¾k âˆˆ Rd , k â‰¤ d, with covariance matrices Î£1 , . . . , Î£k . Denote by Ei the
dispersion ellipsoid of Î¾i :


x
â‰¤
1
, i = 1, . . . , k.
(1.1)
Ei = x = (x1 , . . . , xd ) âˆˆ Rd : x Î£âˆ’1
i
Denote by M the k Ã— d matrix whose rows are Î¾1 , . . . , Î¾k .
Theorem 1.1. The following relation holds:

E det(M M  ) =

(d)k
Vd (E1 , . . . , Ek , B, . . . , B),
(2Ï€)k/2 Îºdâˆ’k

(1.2)

where Vd (Â·, . . . , Â·) denotes the mixed volume of d convex bodies in Rd (see Sec. 2 for details), B is the unit ball
in Rd , (d)k = d(d âˆ’ 1) Â· Â· Â· (d âˆ’ k + 1) is the Pochhammer symbol, and Îºn = Ï€ n/2 /Î“(1 + n/2) denotes the volume
of the unit ball in Rn .
The left-hand side of (1.2) can be interpreted as the average k-dimensional volume of a Gaussian random
parallelotope.
Corollary 1.2. In the case k = d, the following relation holds:
E | det M | =

d!
Vd (E1 , . . . , Ed ).
(2Ï€)d/2

As another direct corollary, we can calculate the mixed volume of d arbitrary ellipsoids in Rd .
Corollary 1.3. If E1 , . . . , Ed are arbitrary ellipsoids defined by symmetric positive definite matrices Î£1 , . . . , Î£d
as in (1.1), then



d
d

1 
1
âˆ’1
(det Î£i )âˆ’1/2
| det(xij )|
exp âˆ’ x
Î£
x
Vd (E1 , . . . , Ed ) =
i dx11 . . . dxdd ,
d! i=1
2 i i
i=1
Rd2

where
xi = (xi1 , . . . , xid ) .
âˆ— St.Petersburg Department of the Steklov Mathematical Institute, St.Petersburg, Russia, e-mail: zap1979@gmail.com.
â€  Ulm University, Ulm, Germany, e-mail: zakhar.kabluchko@uni-ulm.de.

Translated from Zapiski Nauchnykh Seminarov POMI, Vol. 408, 2012, pp. 187â€“196. Original article submitted October
10, 2012.
168
1072-3374/14/1992-0168 Â©2014 Springer Science+Business Media New York

The only estimate of the mixed volume of ellipsoids which we know is due to Barvinok [2]. He showed that
Îºd
(dâˆ’1)/2
3

Dd (Î£1 , . . . , Î£d ) â‰¤ Vd (E1 , . . . , Ed ) â‰¤ Îºd

Dd (Î£1 , . . . , Î£d ),

where Dd (Â·, . . . , Â·) denotes the mixed discriminant of d symmetric d Ã— d matrices:
Dd (A1 , . . . , Ad ) =

âˆ‚d
1
det(Î»1 A1 + Â· Â· Â· + Î»d Ad )
.
d! âˆ‚Î»1 . . . âˆ‚Î»d
Î»1 =Â·Â·Â·=Î»d =0

If Î¾1 , . . . , Î¾k are independent, standard Gaussian vectors, then M M  is a Wishart matrix, and (1.2) turns
into (see [5, 10])

(d)k Îºd
.
E det(M M  ) =
(2Ï€)k/2 Îºdâˆ’k
1.2. Intrinsic volumes. If Î¾1 , Î¾2 , . . . , Î¾k âˆˆ Rd , k â‰¤ d, are identically distributed with the common covariance
matrix Î£ and dispersion ellipsoid E, then (1.2) turns into

k!
Vk (E),
(1.3)
E det(M M  ) =
(2Ï€)k/2
where Vk (Â·) denotes the kth intrinsic volume of a convex body in Rd :
Vk (K) =

d
k

Îºdâˆ’k

Vd (K, . . . , K , B, . . . , B).
 
k times

The normalization is chosen so that Vk (K) depends only on K and not on the dimension of the surrounding
space, i.e., if dim K < d, then the computation of Vk (K) in Rd leads to the same result as the computation in
the aï¬ƒne span of K. In particular, if dim K = k, then Vk (K) = Volk (K), the k-dimensional volume of K.
It is known that V1 (K) is proportional to the mean width of K:
V1 (K) =

dÎºd
w(K).
2Îºdâˆ’1

Taking k = 1 in (1.3), we see that for any centered Gaussian vector Î¾ with dispersion ellipsoid E,
1
EÎ¾ = âˆš V1 (E).
2Ï€

(1.4)

It was pointed out by M. Lifshits that (1.4) is a special case of the following remarkable result of Sudakov.
1.3. Connection with Sudakovâ€™s result. For our purposes, the following ï¬nite-dimensional version of Sudakovâ€™s theorem suï¬ƒces. The result in full generality can be found in [9, Proposition 14].
Proposition 1.4. For an arbitrary subset A âŠ‚ Rd ,
1
E sup x, Î· = âˆš V1 (conv(A)),
2Ï€
xâˆˆA

(1.5)

where Î· is a standard Gaussian vector in Rd and conv(A) is the convex hull of A.
Let us deduce (1.4) from (1.5). Consider a matrix U such that Î£ = U âˆ’1 (U âˆ’1 ) and U Î¾ is a standard Gaussian
vector. Using (1.5) with A = E and Î· = U Î¾, we get
EÎ¾ = E sup x, Î¾ = E sup (U âˆ’1 ) x, U Î¾ = E
xâ‰¤1

xâ‰¤1

1
x, U Î¾ = E supx, U Î¾ = âˆš V1 (E).
2Ï€
xâˆˆE
U  xâ‰¤1
sup

169

1.4. Zeros of Gaussian random fields. Let X(t) = (X1 (t), . . . , Xk (t)) : Rd â†’ Rk , k â‰¤ d, be a random
ï¬eld. Following AzaÄ±Ìˆs and Wschebor [1], we always assume that the following conditions hold:
(a) X is Gaussian;
(b) almost surely, the function X(Â·) is of class C 1 ;
(c) for all t âˆˆ Rd , X(t) has a nondegenerate distribution;
(d) almost surely, if X(t) = 0, then X  (t), the Jacobian matrix of X(t), has the full rank.
Then, almost surely, the level set X âˆ’1 (0) is a C 1 -manifold of dimension d âˆ’ k, and for any Borel set F , the
Lebesgue measure Voldâˆ’k (X âˆ’1 (0) âˆ© F ) is well-deï¬ned (Vol0 (Â·) denotes the counting measure).
It was shown in [1, p. 177] that



E Voldâˆ’k (X âˆ’1 (0) âˆ© F ) = E
det (X  (t)X  (t) ) X(t) = 0 pX(t) (0) dt,
(1.6)
F

where pX(t) (Â·) is the density of X(t). Thus, the integrand in (1.6) can be interpreted as the intensity of zeros
of X.
In this paper, we consider the special case where X is centered and its coordinates X1 , . . . , Xk are independent.
Denote by Ei (t) the dispersion ellipsoid of âˆ‡[Xi (t)/ Var Xi (t)].
Theorem 1.5. Let X be a centered random field with independent coordinates defined as above and satisfying
conditions (a)â€“(d). Then

(d)k
E Voldâˆ’k (X âˆ’1 (0) âˆ© F ) =
Vd (E1 (t), . . . , Ek (t), B, . . . , B) dt.
(1.7)
(2Ï€)k Îºdâˆ’k
F

Formula (1.7) relates zero sets of random equations to mixed volumes. In the case k = d, it is therefore
reminiscent of the well-known fact from the algebraic geometry which we formulate in the next subsection.
1.5. Bernsteinâ€™s theorem. Consider a complex polynomial in d variables,

f (z1 , . . . , zd ) =
cj1 ,...,jd z1j1 . . . zdjd .
The Newton polytope of f is the subset of Rd deï¬ned as follows:


Nw(f ) = conv (j1 , . . . , jd ) âˆˆ Zd : cj1 ,...,jd = 0 .
Let K1 , . . . , Kd be compact convex polytopes in Rd with vertices in Zd . Consider a system of algebraic
equations
âŽ§
âŽª
âŽ¨f1 (z1 , . . . , zd ) = 0,
...
âŽª
âŽ©
fd (z1 , . . . , zd ) = 0,
such that Nw(fi ) = Ki . Bernstein showed [3] that for almost all such systems (with respect to Lebesgue measure
in the space of coeï¬ƒcients of the polynomials), the number of nonzero solutions is equal to
Vol0 (f1âˆ’1 (0) âˆ© Â· Â· Â· âˆ© fdâˆ’1 (0) \ {0}) = d!Vd (K1 , . . . , Kd ).
2. Some essential tools from geometry
For the basic facts from integral and convex geometry we refer the reader to [4] and [8].
2.1. Mixed volumes. Consider arbitrary convex bodies K1 , . . . , Kd âŠ‚ Rd . Minkowski showed [7] that
Vold (Î»1 K1 + Â· Â· Â· + Î»d Kd ), where Î»1 , . . . , Î»d â‰¥ 0, is a homogeneous polynomial of degree d with nonnegative
coeï¬ƒcients:
d
d


Vold (Î»1 K1 + Â· Â· Â· + Î»d Kd ) =
Â·Â·Â·
Î»i1 . . . Î»id Vd (Ki1 , . . . , Kid ).
(2.1)
i1 =1

id =1

The coeï¬ƒcients Vd (Ki1 , . . . , Kid ) are uniquely determined by the assumption that they are symmetric with
respect to permutations of Ki1 , . . . , Kid . The coeï¬ƒcient Vd (K1 , . . . , Kd ) is called the mixed volume of K1 , . . . , Kd .
Diï¬€erentiating (2.1), we get an alternative deï¬nition of the mixed volume:
Vd (K1 , . . . , Kd ) =
170

âˆ‚d
1
Vold (Î»1 K1 + Â· Â· Â· + Î»d Kd ) Î» =Â·Â·Â·=Î» =0 .
1
d
d! âˆ‚Î»1 . . . âˆ‚Î»d

For any aï¬ƒne transformation L,
Vd (LK1 , . . . , LKd ) = | det L| Â· Vd (K1 , . . . , Kd ).
The following relation can also be stated:

Îºdâˆ’1
Vdâˆ’1 (Pu K1 , . . . , Pu Kdâˆ’1 ) du =
Vd (K1 , . . . , Kdâˆ’1 , B),
Îºd

(2.2)

(2.3)

Sdâˆ’1

where du is the surface measure on Sdâˆ’1 normalized to have total mass 1 and Pu denotes the orthogonal projection
to the linear hyperplane uâŠ¥ .
2.2. Volumes of parallelotopes. For any A âŠ‚ Rd and x1 , . . . , xk âˆˆ Rd denote by Px1 ,...,xk A the orthogonal
projection of A to spanâŠ¥ {x1 , . . . , xk } (the orthogonal complement of the linear span of x1 , . . . , xk ). Denote by
Hx1 ,...,xk the parallelotope generated by the vectors x1 , . . . , xk . It is known that

(2.4)
Volk (Hx1 ,...,xk ) = det(AA ),
where A is the matrix whose rows are x1 , . . . , xk .
For any x1 , . . . , xd âˆˆ Rd and k = 1, . . . , d âˆ’ 1,
Vold (Hx1 ,...,xd ) = Volk (Hx1 ,...,xk ) Voldâˆ’k (Px1 ,...,xk Hxk+1 ,...,xd ).

(2.5)

2.3. Ellipsoids. There is a bijection A â†’ E between d Ã— d symmetric positive deï¬nite matrices and ddimensional nondegenerate ellipsoids centered at the origin (see [6] for details):


E = x âˆˆ Rd : x Aâˆ’1 x â‰¤ 1 .
Any nondegenerate linear coordinate transformation of the form x â†’ Lx is reï¬‚ected by a change of the corresponding representing matrix A to the matrix AL given by
AL = LAL .

(2.6)

Let E  be the orthogonal projection of E onto an k-dimensional subspace with some orthonormal basis
x1 , . . . , xk âˆˆ Rd . Denote by A the k Ã— k matrix representing the ellipsoid E  in this basis. If C is the k Ã— d
matrix whose rows are x1 , . . . , xk , then
A = CAC  .
(2.7)
3. Proofs
3.1. Proof of Theorem 1.1. Case k = d. We proceed by induction on d. First let us assume that Î¾d is a standard
Gaussian vector. Denote by Ï‡d a random variable having the chi distribution with d degrees of freedom and
independent from Î¾1 , . . . , Î¾dâˆ’1 . Using (2.4) and (2.5) with k = 1, we get the relations

E Vold (HÎ¾1 ,...,Î¾dâˆ’1 ,Ï‡d u ) du
E | det M | = E Vold (HÎ¾1 ,...,Î¾d ) =
Sdâˆ’1



= EÏ‡d

E Voldâˆ’1 (Pu HÎ¾1 ,...,Î¾dâˆ’1 ) du

Sdâˆ’1

=âˆš

dÎºd
2Ï€Îºdâˆ’1


E Voldâˆ’1 (HPu Î¾1 ,...,Pu Î¾dâˆ’1 ) du.
Sdâˆ’1

It follows from (2.7) that Pu Î¾i has dispersion ellipsoid Pu Ei . By the induction assumption,
E Voldâˆ’1 HPu Î¾1 ,...,Pu Î¾dâˆ’1 =

(d âˆ’ 1)!
Vdâˆ’1 (Pu E1 , . . . , Pu Edâˆ’1 ).
(2Ï€)(dâˆ’1)/2

Combining the latter two relations with (2.3), we obtain the equality
E | det M | =

d!
Vd (E1 , . . . , Edâˆ’1 , B).
(2Ï€)d/2

(3.1)
171

If Î¾d is an arbitrary nondegenerate Gaussian vector, then there exists a linear transformation L such that LÎ¾d
is a standard Gaussian vector. It follows from (2.6) that LEi is the dispersion ellipsoid of LÎ¾i , and, in particular,
LEd = B. Applying (3.1) to the matrix LM  and using (2.2), we get the equalities
d!
| det L|âˆ’1 Vd (LE1 , . . . , LEdâˆ’1 , B)
(2Ï€)d/2
d!
Vd (E1 , . . . , Edâˆ’1 , Ed ).
=
(2Ï€)d/2

E | det M | = | det L|âˆ’1 E | det LM  | =



3.2. Proof of Theorem 1.1. Case k < d. Consider a d Ã— d matrix M  whose ï¬rst k rows form the matrix M and
the last d âˆ’ k rows are independent standard Gaussian vectors Î¾k+1 , . . . , Î¾d (independent from M ). By the
previous case,
d!
Vd (E1 , . . . , Ek , B, . . . , B).
E | det M  | =
(2Ï€)d/2
On the other hand, by (2.5),
E | det M  | = E Vold (HÎ¾1 ,...,Î¾d ) = E Volk (HÎ¾1 ,...,Î¾k ) Voldâˆ’k (PÎ¾1 ,...,Î¾k HÎ¾k+1 ,...,Î¾d )

= E det(M M  ) E Voldâˆ’k (HÎ·1 ,...,Î·dâˆ’k ),
where Î·1 , . . . , Î·dâˆ’k are independent, standard Gaussian vectors in Rdâˆ’k . By the previous case,
E Voldâˆ’k (HÎ·1 ,...,Î·dâˆ’k ) =

(d âˆ’ k)!
Îºdâˆ’k .
(2Ï€)(dâˆ’k)/2

Combining the latter three relations completes the proof.



3.3. Proof of Theorem 1.5. First we assume that Xj has a unit variance: Var Xj (t) â‰¡ 1 for all j = 1, . . . , k.
Diï¬€erentiating the relation EXj (t)Xj (t) = 1 with respect to ti , we obtain the equality
âˆ‚Xj
(t)Xj (t) = 0,
âˆ‚ti
which, together with the independence of the coordinates of X, implies that X  (t) and X(t) are independent.
This means that the conditioning on X(t) = 0 in (1.6) may be dropped. To complete the proof of the theorem
in the case Var Xj (t) â‰¡ 1, it remains to combine (1.6) with (1.2).
To cover the general case, it suï¬ƒces to note that Xj / Var Xj has the same zero set as Xj .

E

Acknowledgments. We are grateful to M. Lifshits for bringing our attention to Sudakovâ€™s result. We are
also grateful to A. I. Barvinok for the useful discussion.
The second author is partially supported by the RFBR (project 10-01-00242), the Program â€œLeading Scientiï¬c
Schoolsâ€ (project-1216.2012.1), and DFG (grant 436 RUS 113/962/0-1 R).
Translated by D. Zaporozhets and Z. Kabluchko.
REFERENCES
1. J. M. AzaÄ±Ìˆs and M. Wschebor, Level Sets and Extrema of Random Processes and Fields, Wiley (2009).
2. A. Barvinok, â€œComputing mixed discriminants, mixed volumes, and permanents,â€ Discrete Comput. Geom.,
18, 205â€“237 (1997).
3. D. N. Bernshtein, â€œThe number of roots of a system of equations,â€ Funct. Anal. Appl., 9, 183â€“185 (1975).
4. Yu. D. Burago and V. A. Zalgaller, Geometric Inequalities, Grundlehren der Mathematischen Wissenschaften,
Vol. 285, Springerâ€“Verlag, Berlin (1988),
5. N. R. Goodman, â€œThe distribution of the determinant of a complex Wishart distributed matrix,â€ Ann. Math.
Statist., 34, 178â€“180 (1963).
6. W. C. Karl, G. C. Verghese, and A. S. Willsky, â€œReconstructing ellipsoids from projections,â€ CVGIP:
Graphical Model and Image Processing, 56, 124â€“139 (1994).
7. H. Minkowski, â€œTheorie der konvexen KoÌˆrper, insbesondere BegruÌˆndung ihres Oberï¬‚aÌˆchenbegriï¬€s,â€ in:
Gesammelte Abhandlungen, Vol. 2 (1911), pp. 131â€“229.
8. R. Schneider and W. Weil, Stochastic and Integral Geometry, Springerâ€“Verlag (2008).
172

9. V. N. Sudakov, â€œGeometric problems in the theory of inï¬nite-dimensional probability distributions,â€ Trudy
Mat. Inst. Akad. Nauk SSSR, 141 (1976).
10. S. S. Wilks, â€œMoment-generating operators for determinants of product moments in samples from a normal
system,â€ Ann. Math., 35, 312â€“340 (1934).

173

